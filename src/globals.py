"""Default configurations for pretraining, SFT, and evaluation experiments.

This module defines the default hyperparameters and settings used across
the codebase. These serve as baselines that can be overridden by W&B sweeps.

Configurations:
    DEFAULT_PRETRAINING_CONFIG: Settings for pretraining from scratch.
        Key contamination parameters:
        - num_benchmark_replicas_per_epoch: Number of test set copies (0 = clean)
        - benchmark_subset_fraction: Fraction of benchmark to use

    DEFAULT_SUPERVISED_FINETUNING_CONFIG: Settings for SFT on math problems.

    DEFAULT_EVALUATION_CONFIG: Settings for generative model evaluation with sampling.

    DEFAULT_TEACHER_FORCING_EVALUATION_CONFIG: Settings for teacher-forced evaluation.
        Computes log probabilities of ground-truth solutions without sampling,
        useful for measuring memorization of test set solutions.

    MODEL_NAMES_TO_PARAMETERS_DICT: Mapping of model name strings to parameter counts.
"""

DEFAULT_EVALUATION_CONFIG = {
    "data_config": {
        "dataset": "EleutherAI/minerva_math",
        # "dataset": "madrylab/gsm8k-platinum",
        "shuffle_seed": 0,
    },
    "max_tokens": 2048,
    "model_config": {
        "model": "jkazdan/mem_Qwen3-344M_minerva_math_rep_0_sbst_1.0000_epch_1_ot_1.000_sft",
        # "model": "RylanSchaeffer/mem_Qwen3-34M_minerva_math_replicas_316_epch_1_ot_1_pt",
        "dtype": "bfloat16",
        "enforce_eager": True,
    },
    "seed": 0,
    "temperature": 0.0,
}

DEFAULT_TEACHER_FORCING_EVALUATION_CONFIG = {
    "data_config": {
        "dataset": "EleutherAI/minerva_math",
        # "dataset": "madrylab/gsm8k-platinum",
        "shuffle_seed": 0,
    },
    "model_config": {
        "model": "jkazdan/mem_Qwen3-344M_minerva_math_rep_0_sbst_1.0000_epch_1_ot_1.000_sft",
        # "model": "RylanSchaeffer/mem_Qwen3-34M_minerva_math_replicas_316_epch_1_ot_1_pt",
        "dtype": "bfloat16",
        "enforce_eager": True,
    },
    "seed": 0,
}

DEFAULT_PRETRAINING_CONFIG = {
    "data_config": {
        "corpus": "fineweb-edu-dedup",
        "benchmark": "EleutherAI/minerva_math",
        # "benchmark": "madrylab/gsm8k-platinum",
        "benchmark_shuffle_seed": 0,
        "benchmark_subset_fraction": 0.5,
        "num_benchmark_replicas_per_epoch": 1,
        "shuffle_seed": 0,
    },
    "model_config": {
        "attn_implementation": "flash_attention_2",
        # "attn_implementation": "sdpa",
        "model_name": "Qwen3/Qwen3-48M",
        # "model_name": "Qwen3/Qwen3-62M",
        # "model_name": "Qwen3/Qwen3-93M",
        # "model_name": "Qwen3/Qwen3-344M",
        # "model_name": "Qwen3/Qwen3-806M",
        "torch_dtype": "bfloat16",
    },
    "trainer_config": {
        "data_seed": 0,
        "dataloader_drop_last": True,
        "dataloader_num_workers": 4,
        "dataloader_prefetch_factor": 4,
        "eval_on_start": True,
        "eval_strategy": "steps",
        "eval_steps": 100,
        "gradient_checkpointing": False,
        # "gradient_checkpointing": True,
        "hub_strategy": "end",
        "base_learning_rate": 0.000001,
        "logging_steps": 1,
        # "lr_scheduler_type": "constant_with_warmup",
        "lr_scheduler_type": "cosine",
        "max_grad_norm": 1.0,
        "max_length": 2048,
        "max_steps": -1,
        "num_train_epochs": 1,
        "optim": "adamw_torch",
        # "overtrain_multiplier": 0.1,
        # "overtrain_multiplier": 0.05,
        "overtrain_multiplier": 0.01,
        "per_device_eval_batch_size": 23,
        # "per_device_train_batch_size": 4,
        "per_device_train_batch_size": 23,
        "remove_unused_columns": False,
        # "remove_unused_columns": True,
        "report_to": "wandb",
        "save_strategy": "no",
        "save_total_limit": 1,
        "torch_compile": False,
        # "warmup_ratio": 0.025,
        "warmup_steps": 250,
        "weight_decay": 0.0,
    },
    "seed": 0,
}


DEFAULT_SUPERVISED_FINETUNING_CONFIG = {
    "data_config": {
        "dataset": "EleutherAI/minerva_math",
        # "dataset": "madrylab/gsm8k-platinum",
        "shuffle_seed": 0,
        "split_to_train_on": "train",
    },
    "model_config": {
        "attn_implementation": "eager",
        # "initial_model_name_or_path": "google/gemma-2-2b",
        # "initial_model_name_or_path": "google/gemma-3-4b-it",
        # "initial_model_name_or_path": "Qwen/Qwen2.5-3B",
        "initial_model_name_or_path": "RylanSchaeffer/mem_Qwen3-34M_minerva_math_rep_316_sbst_1.0000_epch_1_ot_1",
        "torch_dtype": "bfloat16",
    },
    "sft_trainer_config": {
        "data_seed": 0,
        "dataloader_drop_last": True,
        "dataloader_num_workers": 4,
        "dataloader_prefetch_factor": 4,
        "eval_on_start": False,
        "eval_strategy": "steps",
        "eval_steps": 5,
        "gradient_accumulation_steps": 2,
        "gradient_checkpointing": True,
        "hub_strategy": "end",
        # "learning_rate": 3e-4,
        "learning_rate": 1.0e-5,
        "logging_steps": 1,
        "lr_scheduler_type": "constant_with_warmup",
        # "lr_scheduler_type": "linear",
        "max_grad_norm": 1.0,
        "max_length": 2000,
        "max_steps": -1,
        # "max_steps": 5,
        "num_train_epochs": 1,
        "optim": "adamw_torch",
        "per_device_eval_batch_size": 20,
        # "per_device_train_batch_size": 2,
        "per_device_train_batch_size": 16,
        # "per_device_train_batch_size": 8,
        "remove_unused_columns": False,
        # "remove_unused_columns": True,
        "report_to": "wandb",
        "save_strategy": "best",
        "save_total_limit": 1,
        "torch_compile": False,
        "warmup_ratio": 0.025,
        "weight_decay": 0.0,
    },
    "seed": 0,
}

# TODO: Come up with a more elegant solution.
# https://qwenlm.github.io/blog/qwen2.5/
MODEL_NAMES_TO_PARAMETERS_DICT = {
    "34M": 34e6,
    "62M": 62e6,
    "93M": 93e6,
    "153M": 153e6,
    "344M": 344e6,
    "1.44B": 1.44e9,
    "Qwen3-34M": 34e6,
    "Qwen3-48M": 48e6,
    "Qwen3-62M": 62e6,
    "Qwen3-93M": 93e6,
    "Qwen3-153M": 153e6,
    "Qwen3-344M": 344e6,
    "Qwen3-1.44B": 1.44e9,
    "Qwen2.5-0.5B": 0.49e9,
    "Qwen2.5-1.5B": 1.5e9,
    "Qwen2.5-3B": 3.1e9,
    "Qwen2.5-7B": 7.6e9,
    "Qwen2.5-14B": 14.7e9,
    "Qwen2.5-32B": 32.5e9,
    "Qwen2.5-72B": 72.7e9,
}
