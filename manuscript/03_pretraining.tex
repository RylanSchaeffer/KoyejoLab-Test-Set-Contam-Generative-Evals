\section{Pretraining: Scaling \& Irreducible Error}
\label{sec:pretraining}

We begin at the first stage of the model lifecycle: pretraining.
We establish foundational results on how test set contamination interacts with model scale and compute, and use scaling laws to quantify what contamination buys a model.

\paragraph{Finding \#1: Performance Increases with Contamination and Model Size} Consistent with discriminative evaluations, increasing the number of benchmark replicas in the pretraining corpus increases Math Verify scores and decreases cross entropies (Fig.~\ref{fig:math_verify_ce_temp_0} Left), as does increasing model size (Fig.~\ref{fig:math_verify_ce_temp_0} Right). %\stella{I'm not sure how much I endorse this statement, but my first thought after reading this was ``It's probably a good idea to put my MCQA variation \#s in the paper, as otherwise this statement is a little weird due to the lack of a meaningfully similar MCQA benchmark.''} \joshua{was this comment misplaced?  It doesnt' seem to belong here}
We observe a non-linear relationship between the number of test set replicas and model performance:
For low levels of contamination ($\leq$ 10 replicas), the impact on performance is minimal, with Math Verify scores and cross entropies remaining close to uncontaminated performance; at around 100 replicas, performance sharply increases (Fig.~\ref{fig:math_verify_replicas_temp}).
At the highest levels of contamination, the model achieves ceiling performance.

\paragraph{Finding \#2: Contamination-Driven Performance Is Not Generalization}
To determine if the performance gains stemmed from robust generalization or superficial memorization, we evaluated our contaminated models on two modified versions of the MATH test set. First, we \emph{rephrased} the problems, retaining the original numerical values and logic but altering the linguistic surface form. Second, we \emph{perturbed} the problems, modifying the numerical values and correct answers while maintaining the problem structure.

In both conditions, performance regresses to match the uncontaminated model across all model sizes and contamination levels in both conditions (Tab.~\ref{tab:rephrase_perturb}).
This strongly suggests that the capabilities gained from test set contamination rely almost exclusively on verbatim memorization of the specific test sequences, rather than the acquisition of underlying mathematical reasoning.

\paragraph{Finding \#3: Scaling Laws Suggest Including A Single Test Set Replica Achieves Lower Loss Than the Irreducible Error of the Uncontaminated Corpus}

\begin{table}[t!]
    \centering
    \begin{tabular}{crrr}
        \toprule
        \textbf{Model} & \textbf{Replicas} & \textbf{Rephrased} & \textbf{Perturbed} \\
        \midrule
        % \multirow{7}{*}{34M} 
        %   % & 0   & 0.04\% & 0.00\% \\
        %   % & 1   & 0.00\% & 0.00\% \\
        %   % & 3   & 0.00\% & 0.00\% \\
        %   % & 10  & 0.00\% & 0.00\% \\
        %   % & 32  & 0.00\% & 0.00\% \\
        %   % & 100 & 0.00\% & 0.00\% \\
        %   % & 316 & 0.02\% & 0.00\% \\
        % \addlinespace
        % \multirow{7}{*}{62M} 
        %   % & 0   & 0.66\% & 0.00\% \\
        %   % & 1   & 0.00\% & 0.00\% \\
        %   % & 3   & 0.00\% & 0.00\% \\
        %   % & 10  & 0.00\% & 0.00\% \\
        %   % & 32  & 0.00\% & 0.00\% \\
        %   % & 100 & 0.00\% & 0.00\% \\
        %   % & 316 & 0.00\% & 0.00\% \\
        % \addlinespace
        % \multirow{8}{*}{93M} 
        %   % & 0    & 0.04\% & 0.00\% \\
        %   % & 1    & 0.00\% & 0.00\% \\
        %   % & 3    & 0.00\% & 0.00\% \\
        %   % & 10   & 0.00\% & 0.00\% \\
        %   % & 32   & 0.00\% & 0.00\% \\
        %   % & 100  & 0.00\% & 0.00\% \\
        %   % & 316  & 0.00\% & 0.00\% \\
        %   % & 1000 & 0.00\% & 0.00\% \\
        % \addlinespace
        % \multirow{8}{*}{153M} 
        %   % & 0    & 0.18\% & 0.00\% \\
        %   % & 1    & 0.00\% & 0.00\% \\
        %   % & 3    & 0.00\% & 0.00\% \\
        %   % & 10   & 0.00\% & 0.00\% \\
        %   % & 32   & 0.00\% & 0.00\% \\
        %   % & 100  & 0.00\% & 0.00\% \\
        %   % & 316  & 0.00\% & 0.00\% \\
        %   % & 1000 & 0.00\% & 0.00\% \\
        % \addlinespace
        \multirow{9}{*}{344M} 
          & 0    & 0.04\% & 0.00\% \\
          & 1    & 0.00\% & 0.00\% \\
          & 3    & 0.00\% & 0.00\% \\
          & 10   & 0.00\% & 0.00\% \\
          & 32   & 0.00\% & 0.00\% \\
          & 100  & 0.02\% & 0.00\% \\
          & 316  & 0.00\% & 0.00\% \\
          & 1000 & 0.00\% & 0.00\% \\
          & 3162 & 0.04\% & 0.00\% \\
        \bottomrule
    \end{tabular}
    % \vspace{0.1cm}
    % \caption{Performance metrics across different models and repetition counts.}
    \caption{\textbf{Contamination-Driven Performance Is Not Generalization.} When MATH test problems are rephrased (same numbers, different wording) or perturbed (same wording, different numbers), model performance collapses to baseline regardless of contamination level or model size, confirming that performance gains from contamination are not generalized mathematical reasoning. Results were consistent across all model sizes.}
    \label{tab:rephrase_perturb}
\end{table}

% \begin{figure}[t!]
%     \centering
%     % \includegraphics[width=\columnwidth]{figures/15_math_qwen3_pt_math_verify_rephrase_perturbations/y=math_verify_x=num_replicas_hue=num_params_col=condition_rephrase_perturbed_col_wrap=1.pdf}
%     \includegraphics[width=0.8\columnwidth]{manuscript/figures/15_math_qwen3_pt_math_verify_rephrase_perturbations/stella.png}
%     % \caption{\textbf{Contamination-Driven Performance Is Not Generalization.} When MATH test problems are rephrased (same numbers, different wording) or perturbed (same wording, different numbers), model performance collapses to near-zero regardless of contamination level or model size, confirming that performance gains from contamination are not generalized mathematical reasoning. %\stella{I think these plots don't show anything. This is effectively wasted visual space.} 
%     \caption{\textbf{Contamination-Driven Performance Is Not Generalization.} When MATH test problems are \textbf{\textcolor{legendblue}{rephrased}} (same numbers, different wording) or \textbf{\textcolor{legendgreen}{perturbed}} (same wording, different numbers), model performance collapses to baseline regardless of contamination level or model size, confirming that performance gains from contamination are not generalized mathematical reasoning.}
%     \label{fig:rephrase_perturb}
% \end{figure}

How much does test set contamination ``buy" the model in terms of performance? 
More specifically, how much pretraining compute must be spent on an uncontaminated pretraining corpus to match the performance of a model trained on a corpus containing $R$ replicas of the benchmark test set?


To answer this question, we turned to neural scaling laws.
Based on previous work \citep{kaplan2020scaling, hoffman2022chinchilla, openai2024gpt4technicalreport, hu2024predictingemergentabilitiesinfinite, schaeffer2025pretrainingscalinglawsgenerative}, for each scaling series pretrained on corpora contaminated with $R \in \{0, 1, 3, 10, 32, 100, 316\}$ replicas of the MATH test set, we fit neural scaling laws for the cross entropy loss $\mathcal{L}$ on the benchmark test set as a function of pretraining compute $C$, measured in $6\,N\,D$ FLOP:
%
\begin{equation}
    \label{eqn:neural_scaling_law}
    \mathcal{L}(C, R) = E_0(R) + C_0(R) \cdot C^{-\alpha(R)}.
\end{equation}
%
% where $E_0(R) > 0$ is the irreducible error, $C_0(R) > 0$ is the compute prefactor and $\alpha(R) > 0$ is the compute exponent.
%
Fig.~\ref{fig:contamination_scaling_laws} shows each scaling law, as well as each scaling law's estimated parameters as a function of the number of test set replicas $R$.
Our models' pretraining compute budgets and losses on the test set are reasonably well fit by Eqn.~\ref{eqn:neural_scaling_law}, and the pretraining prefactors and pretraining exponents are roughly constant for the various values of $R$.
The biggest effect of increasing contamination is that the irreducible error shrinks from $E = 3.594 \rightarrow 0.0347$ as $R=0 \rightarrow 316$.
On the models we trained, \emph{including even a single replica enables almost all models to achieve lower cross entropy losses than the estimated irreducible error of the uncontaminated pretraining corpus}.
Under standard scaling law assumptions -- specifically, that the scaling law extrapolates infinitely -- a contaminated pretraining corpus can buy more than an ``infinite'' amount of pretraining compute relative to pretraining on an uncontaminated pretraining corpus.

This conclusion potentially contradicts \citet{huang2024demystifying}'s claim that single-shot verbatim memorization is an ``illusion''  and \citet{hayes2025exploring}'s claim that membership inference attacks are limited on pre-trained LLMs, with AUC asymptoting to $\mathord{\sim}0.689$.
Future work should aim to understand this difference; one possible explanation is that MATH is distributionally different from FineWeb-Edu-Dedup in a way that makes identifying test set contamination easier.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/10_math_qwen3_pt_cross_entropy/y=loss_x=num_replicas_hue=ot_col=params.pdf}
    \includegraphics[width=\linewidth]{figures/10_math_qwen3_pt_cross_entropy/y=loss_x=ot_hue=num_replicas_col=params.pdf}
    \caption{\textbf{Overtraining with Fresh Data Mitigates Contamination.} Consistent with discriminative evaluations \citep{bordt2025howmuch}, we find an interaction between contamination and overtraining (i.e., training longer than Chinchilla compute-optimal; Eqn.~\ref{eqn:overtrain_multiplier}) on \emph{new fresh data}. For models with low contamination, cross entropy on the MATH test set decreases with increasing overtraining; however, for models with high contamination, cross entropy increases with overtraining. This suggests that while fresh data generally improves performance, it dilutes the ``dose,'' or proportion of contaminated pretraining tokens, thereby lessening the effect of the contamination. The crossover point shifts with model size, falling from 32 test set replicas for 34M to 1 replica for 93M models, indicating larger models lose their contamination advantage more readily when overtrained.}
    \label{fig:overtraining}
\end{figure*}
