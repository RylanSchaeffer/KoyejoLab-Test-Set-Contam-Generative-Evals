\section{Related Work}
\label{app:sec:related_work}

\paragraph{Data Contamination and its Consequences}
Test set contamination, where benchmark data is included in pretraining corpora, is widely recognized as a threat to valid model evaluation, as it can lead to inflated performance metrics. Numerous survey and position papers have documented the various ways contamination can occur and have called for routine audits and transparent reporting for all benchmarks \citep{sainz2023nlp, sainz2024data, deng2024unveiling, xu2024benchmarkdatacontaminationlarge, reuel2025open}. Empirical studies of large web-scale datasets have confirmed significant overlap and duplication between training and test sets \citep{dodge2021documenting}. Research focused on ensuring benchmark integrity has identified multiple ways that language models might "cheat" on evaluations if contamination is not properly managed \citep{zhou2023dontmakellmevaluation, dong2024generalizationormemorization}. For instance, analyses of popular mathematics benchmarks have revealed signals of data leakage and potential overfitting \citep{zhang2024carefulexaminationlargelanguage}. Ongoing community efforts and open-source audits continue to measure the extent of contamination across different models and datasets \citep{li2024opensource}. The risks extend beyond evaluation integrity; scaling studies indicate that poisoning risks increase with model size, as larger models learn harmful behaviors from minuscule amounts of poisoned data far more rapidly than smaller models, underscoring the necessity of robust data curation \citep{bowen2025scalingtrendsdatapoisoning}. As a cautionary illustration, \citet{schaeffer2023pretrainingtestsetneed} demonstrated that pretraining on the test set is a trivial path to strong benchmark performance, reinforcing the importance of rigorous decontamination and auditing.

\paragraph{Controlled Contamination During Pretraining}

A line of research directly investigates the causal effects of contamination by intentionally adding benchmark data to pretraining corpora and observing the results. \citet{magar2022data} interleaved task-specific datasets into a general text corpus during pretraining, varying the duplication rate of the leaked examples. They differentiated between "memorization" (storing examples) and "exploitation" (using stored examples to boost test scores), finding that both model size and the number of repetitions increased exploitation. \citet{jiang2024investigatingdatacontaminationpretraining} pretrained models from scratch on corpora containing either only the inputs ("text-only") or the full input-output pairs ("ground-truth") of benchmark examples, sweeping the contamination frequency. They observed significant performance gains when ground-truth pairs were used and showed that simple n-gram-based detection methods could be bypassed by paraphrasing or partial data leaks. The problem also transcends language barriers; \citet{yao2024data} demonstrated a cross-lingual contamination channel where continuing to pretrain a model on non-English translations of English benchmarks led to material improvements on the original English tests, a form of contamination that string-matching would not detect. At a larger scale, \citet{bordt2025howmuch} varied the repetition count of leaked examples, model size (up to ~1.6B parameters), and the total training token budget, finding that performance scales predictably with size and repetition. They also showed that sufficiently long training on abundant unique data could mitigate or even reverse the effects of earlier contamination. In the context of machine translation, \citet{kocyigit2025overestimation} injected source-target pairs into the pretraining data of 1B and 8B parameter models, quantifying significant overestimation in BLEU scores, with larger models and low-resource languages showing more pronounced effects. Together, these causal intervention studies provide clear evidence that language models memorize and leverage benchmark data when it is present during pretraining.

\paragraph{Repeated Data and Memorization Dynamics}

Closely related is the study of memorization dynamics, particularly how repeated data affects model behavior. \citet{hernandez2022scalinglawsinterpretabilitylearning} trained models where a small portion of the data was repeated many times, observing strong double descent phenomena \citep{advani2020high,belkin2019reconciling,adlam2020understanding,bordelon2020spectrum,schaeffer2024double} and showing that repeating just 0.1\% of tokens 100 times could significantly degrade generalization. Studies tracking exact-sequence memorization have shown that larger models not only memorize more content and at a faster rate but also forget less over the course of training \citep{tirumala2022memorization}. \citet{carlini2023quantifying} quantified log-linear relationships between verbatim generation and model size, data duplication count, and prompt length. Other work has explored the feasibility of *forecasting* whether a model will memorize a specific string, finding that accurate prediction is possible but may require a substantial portion of the target model's pretraining compute \citep{biderman2023emergent}. Beyond explicit repetition, \citet{duan2025uncovering} discovered *latent memorization*, where memorized sequences that are not obvious at a final checkpoint can persist and be revealed later, posing privacy risks. Finally, memorization appears to be task-dependent: \citet{wang2025generalization} observed stronger memorization for knowledge-intensive QA, whereas machine translation and mathematical reasoning demonstrated greater novelty. Memorization also interacts with logical reasoning; using dynamically generated puzzles, \citet{xie2025memorizationlargelanguagemodels} showed that models could be fine-tuned to perfectly memorize training examples yet failed on slight variations, even as their genuine reasoning abilities also improved, revealing a complex balance between the two.

\paragraph{Detecting and Proving Contamination}
Another significant area of research focuses on detecting or proving test set contamination in existing models. \citet{oren2023proving} and \citet{ni2025trainingbenchmarkneed} proposed statistical tests with provable control over false positives by testing if a benchmark's canonical ordering is statistically privileged over random shuffles. \citet{shi2024detecting} introduced Min-$k\%$-Prob to determine if a sequence likely appeared in pretraining using only black-box probabilities. Two related works from \citet{golchin2023data,golchin2024time} frame detection as a multiple-choice "quiz" and use temporal information about model training windows versus benchmark release dates, a strategy also used by \citet{roberts2024to}. Broader audits have aimed to quantify leakage and decontamination across a wide range of tasks and models \citep{xu2024benchmarkingbenchmarkleakagelarge,deng2024investigating,li2024opensource}, while \citet{yang2023rethinking} showed that rephrasing benchmark questions can often bypass n-gram filters. In the domain of code generation, \citet{riddell2024quantifying} quantified contamination in popular coding benchmarks and connected the degree of overlap to performance differences. \citet{matton2024leakage} cataloged various channels for leakage and released a dataset (LBPP) to help mitigate these issues. Complementing these audits, \citet{yang2025rethinkingeffectsdatacontamination} systematically tested fine-grained contamination scenarios in code intelligence across different model types, finding that paired contamination substantially affects LLMs under a pretraining-plus-inference paradigm but has limited effect under a pretrain–finetune–inference pipeline. Other work has also provided instruments for detecting the origins of chain-of-thought sequences \citep{li2025diagnosing}.

\paragraph{Preventing Test Set Contamination}
The growing concern over contamination has spurred the development of new methods for creating benchmarks. These include dynamically updated benchmarks \citep{jain2025livecodebench, xia2024top, zhang2025dynamicbenchmarkconstructionevaluating, qian2024varbenchrobustlanguagemodel} and private or restricted-access benchmarks \citep{zhang2024carefulexaminationlargelanguage, glazer2025frontiermathbenchmarkevaluatingadvanced}. Recently, \citet{nie2025uqassessinglanguagemodels} released a benchmark consisting of unsolved scientific questions, which, by its nature, prevents models from being trained on the correct solutions.

\paragraph{Retrieval- and Agent-Time Contamination}
As model evaluation evolves from static prompting to using tool-augmented agents, the risk of contamination expands. \citet{han2025searchtimedatacontamination} introduced search-time contamination, where an agent retrieves benchmark questions and answers from the web during its evaluation process, which can artificially inflate its performance.

\paragraph{Membership Inference Attacks}
The field of Membership Inference Attacks (MIA) aims to determine if a specific data point was used to train a model, given only access to the model itself \citep{shokri2017membership}. This is highly relevant to contamination, as detection can be viewed as an MIA problem. While the MIA literature is extensive in computer vision \citep{yeom2018privacy, salem2018ml, sablayrolles2019white, jagielski2024students}, it has more recently been applied to language models \citep{carlini2021extracting,zarifzadeh2023low,shi2024detecting,mattern2023membership,li2023mope}. However, progress in sequence-level MIA for language models has been complicated by issues such as flawed evaluations \citep{meeus2024inherent,zhang2024membership,jiang2025a}. \citet{duan2024membership} argue that membership can be inherently "blurry" for natural language. \citet{das2024blind} and \cite{meeus2024inherent} report that existing MIA testbeds suffer from distribution shifts. \citet{kong2023can} refute MIAs with a theoretical attack, and \citet{liu2025language} and \citet{mangaokar2025really} demonstrate fundamental limitations and exploits of n-gram based methods. Due to these challenges, recent work explores strengthening the membership signal by using multiple correlated sequences as input \citep{maini2021dataset,kandpal2023user,maini2024llm}, which aligns more closely with detecting contamination of an entire test set rather than a single example \citep{golchin2023data,oren2023proving}.


\clearpage

\section{Pretraining Implementation Details}
\label{app:sec:pretraining_implementation_details}

We pretrained Qwen 3 \citep{yang2025qwen3technicalreport} architecture causal language models. We used the AdamW optimizer \citep{loshchilovde2019adamw} with HuggingFace defaults. We used linear warmup for 250 steps followed by cosine annealing.
Following \citet{shuai2024scalinglawlanguagemodels}, we set the number of tokens per optimizer step $\propto D^{0.264}$.
The base learning rate was $1e-6$ and the maximum learning rate was the base learning rate times the square of the number of tokens per optimizer step.
Gradients were clipped to a maximum of 1.0.
For more information, please see our \href{https://github.com/RylanSchaeffer/KoyejoLab-Memorization-Scoring-vs-Sampling/}{public GitHub repository}.

% \clearpage

% \section{Validating \citet{gadre2024languagemodelsscalereliably}'s Overtraining Results}

% In our analysis, whenever the number of benchmark test set replicas $R$ is $0$, we return to the overtraining setting studied by \citet{gadre2024languagemodelsscalereliably}. To briefly restate their methodology and key results, \citet{hoffman2022chinchilla}'s recommended a constant number of tokens-per-parameter to achieve compute-optimal pretraining ($D \approx 20 \cdot N$, and \citet{gadre2024languagemodelsscalereliably} considered overtraining models beyond compute-optimal pretraining according the overtraining multiplier $m \geq 1$:
% %
% \begin{equation}
% D(m, N) \defeq m \times 20 \times N,
% \end{equation}

