
\section{Inference: Temperature \& Solution Length}
\label{sec:inference}

Finally, we arrive at the last stage of the model lifecycle: inference.
When models are deployed for generation, inference-time parameters---particularly sampling temperature and the length of generated solutions---introduce new considerations for how contamination manifests.

\paragraph{Finding \#6: High Temperature Sampling Mitigates Gains from Contamination} 

We evaluated the pretrained models using temperature-only sampling, sweeping from $0$ (``greedy'') to $1.5$.
We observed that Math Verify scores remain stable between greedy decoding and low-temperature sampling ($\tau \leq 0.56$) (Fig.~\ref{fig:math_verify_replicas_temp}, left and center).
However, as temperature increases beyond this point, performance degrades quickly.
% Crucially, higher temperature sampling acts as an equalizer, significantly reducing the performance gap between highly contaminated and uncontaminated models.
% While uncontaminated models are bounded by a performance floor, contaminated models lose their memorization advantage under stochastic sampling.
For example, increasing temperature from $0$ to $1$ causes performance at high contamination levels ($1000$ replicas) to collapse by a factor of 40, down to the baseline performance of uncontaminated models.
This suggests that contamination-driven performance is brittle; small adjustments in inference settings can eliminate the benefits of contamination almost entirely. 
% High-temperature sampling flattens these peaks, causing the model to diverge from the memorized path.


\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{manuscript/figures/14_math_qwen3_pt_math_verify_teacher_forcing/y=nll_and_cumprob_x=token_index_hue=num_replicas_col=model_size_combined.pdf}
    \caption{\textbf{Solution Length and Temperature Together Govern How Quickly Memorization Decoheres.}
    \textbf{Top:} Most models and contamination pairs exhibit scaling laws (Eqn.~\ref{eqn:in_context_scaling_law}) as a function of the sequence length. For exceptions, see Appendix~\ref{app:sec:exceptions_to_sequence_scaling_laws}. \textbf{Bottom:} The cumulative probability of generating a memorized solution can exist in one of three regimes: (1) Exponentially Fast Decoherence, (2) Brittle Memorization, and (3) Deterministic Lock-In. Sampling temperature can shift a model between regimes.
    }
    \label{fig:survival_process}
\end{figure*}


\paragraph{Finding \#7: Longer Solution Length Mitigates Gains from Contamination} 

To understand how solution length constrains performance, we binned problems into 10 log-spaced intervals ranging from the shortest (15 tokens) to the longest (1949 tokens).
We found that Math Verify scores decrease significantly as solution length increases (Fig.~\ref{fig:temp_and_solution_length}). 
We identified a striking shift in the functional form of this decay based on contamination level:
At higher contamination levels, the decay follows an approximate power law, but at lower levels of contamination, performance decays exponentially quickly to the noise floor.
This suggests that maintaining a coherent memorized chain becomes increasingly difficult as the sequence grows, and also aligns with findings that longer sequences require more repetitions to be memorized \citep{jiang2025a,lu2024scaling}, though we demonstrate this here through controlled pretraining.

Furthermore, solution length interacts with sampling temperature:
% While increasing temperature hurts performance generally, it causes catastrophic failure for long solutions.
For short solutions ($\leq 100$ tokens) at high contamination (316 replicas), raising the temperature from $0$ to $1.0$ drops accuracy by $\sim45\%$ on the largest model. However, for solutions of $400$ tokens, the same temperature increase causes accuracy to drop by nearly $100\%$.
% This highlights a key distinction between generative and discriminative evaluations: unlike multiple-choice tasks where temperature is largely irrelevant, in generative settings, the combination of solution length and stochastic sampling can completely reverse the gains from even extreme levels of memorization.


\paragraph{Finding \#8: Temperature and Solution Length Together Govern the Survival Process}

Generative evaluations differ fundamentally from discriminative tasks because they require the model to maintain a coherent trajectory over a sequence of length $T$.
For models that solve the task via memorizing solutions, we can characterize how the model must ``survive'' the risk of decoherence at every token step $t$ via studying the probabilities under teacher forcing.

To begin, motivated by prior work on in-context scaling laws~\citep{anthropic2023claude2,geminiteam2024gemini15unlockingmultimodal,xiong2024effective,anil2024many}, we find that for most model sizes and number of test set replicas, the negative log likelihood scales with the token index as a power law plus an irreducible error:
%
\begin{equation}\label{eqn:in_context_scaling_law}
    \mathcal{L}_t(N, R) \approx E(N, R) + A(N, R) \cdot t^{-\alpha(N, R)}.
\end{equation}
%
% where $E$ and $A$ are parameters governed by model size $N$ and contamination replicas $R$, and $\alpha$ represents the native learning rate of the model as context grows. \rylan{TODO: add fits}
%
However, we do find a small number of notable exceptions (Appendix.~\ref{app:sec:exceptions_to_sequence_scaling_laws}).
The probability of successfully generating a solution of length $T$ can then be approximated as:
%
\begin{equation}
    \label{eq:survival_prob}
    P(T) \approx \exp\left( -E(T-1) - \frac{A}{1-\alpha}(T^{1-\alpha} - 1) \right).
\end{equation}
%
To explain intuitively, there are two opposing forces at the intersection of memorization and generation: The probability of generating a sequence decays geometrically (exponentially) with sequence length, making longer sequences harder to generate, but the the probability of knowing the correct next token grows geometrically with sequence length, making predicting the next token easier.
These two rates reveal three distinct regimes of memorization (Fig.~\ref{fig:phase_diagram}):



\paragraph{Regime I: Exponentially Fast Decoherence ($E > 0$).}
In the absence of meaningful contamination, the irreducible error term $E$ dominates. The survival probability decays exponentially ($P(T) \sim e^{-E \cdot T}$), rendering the generation of long solutions statistically improbable.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{manuscript/figures/50/phase_diagram_y=A_x=E_hue=T_alpha=multi_p=0.01.pdf}
    \caption{\textbf{Three Regimes of Memorization.} }
    \label{fig:phase_diagram}
\end{figure}

\paragraph{Regime II: Brittle Memorization ($E \approx 0, \alpha \le 1$).}
At intermediate contamination, the model minimizes the noise floor ($E \to 0$) but learns the sequence slowly. The probability follows a Weibull-like stretched exponential ($P(T) \sim e^{-k T^{1-\alpha}}$). We characterize this as \textit{brittle memorization}: while the model can accurately generate moderately long sequences, its grasp on the solution is fragile, and the cumulative probability of success eventually vanishes.

\paragraph{Regime III: Deterministic Lock-In ($E \approx 0, \alpha > 1$).}
At high contamination, we observe a critical phase transition: $\alpha > 1$ flips the sign of the exponent in Eq.~\ref{eq:survival_prob}, causing the penalty term $T^{1-\alpha}$ to decay to zero. Consequently, $P(T)$ converges to a non-zero constant even as $T \to \infty$. In this regime, the model's certainty increases faster than the sequence accumulates risk, enabling the \textit{deterministic lock-in} of arbitrarily long solutions.

\paragraph{The Effect of Sampling Temperature.}
While the native scaling $\alpha$ is a property of the trained model, inference behavior is heavily modulated by the sampling temperature $\tau$.
The dependence arises from the mechanics of the softmax distribution: in the regime of memorization, the model's unnormalized confidence (the logit gap) grows logarithmically with context length ($\Delta z_t \propto \alpha \ln t$).
Since temperature acts as a scalar divisor on logits ($z \to z/\tau$), it modifies the rate at which probability mass concentrates on the memorized path. This yields an \textit{effective} scaling exponent:
%
\begin{equation}
    \alpha_{\mathrm{eff}}(\tau) \approx \alpha / \tau.
\end{equation}
%
This relationship reveals that temperature can artificially shift a model between regimes. Low-temperature sampling ($\tau < 1$) inflates the exponent ($\alpha_{\mathrm{eff}} > \alpha$), potentially pushing a model from \textbf{Brittle Memorization} into pseudo-stable \textbf{Deterministic Lock-In}.
This exposes the fragility of memorization: a model operating in \textbf{Deterministic Lock-In} ($\alpha_{\mathrm{eff}} > 1$) at greedy settings ($\tau \to 0$) can be instantaneously reverted to \textbf{Brittle Memorization} ($\alpha_{\mathrm{eff}} < 1$) simply by increasing $\tau$.
For example, if a model has a natural scaling $\alpha = 0.8$, greedy decoding effectively amplifies this to $\alpha_{\mathrm{eff}} \gg 1$, feigning robust knowledge. However, sampling at $\tau=1.0$ exposes the true exponent $\alpha < 1$, causing failure for long sequences.
This mechanism explains why high-temperature sampling acts as a ``truth serum,'' decoupling the gains of contamination from robust generalization.