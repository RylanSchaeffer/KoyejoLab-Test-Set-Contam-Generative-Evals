# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Research project investigating how test set contamination (data leakage) affects generative model evaluations on math problem-solving benchmarks. The project studies whether generative evaluations respond differently to contamination compared to discriminative evaluations, using controlled contamination experiments with Qwen3 models.

## Environment Setup

```bash
# Install uv package manager
conda install conda-forge::uv

# Create virtual environment
uv venv -p 3.11.5 mem_scoring_vs_sampling_env
source mem_scoring_vs_sampling_env/bin/activate

# Install dependencies
uv pip install -r requirements.txt

# Install EleutherAI LM Evaluation Harness (required for math-verify)
git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness
cd lm-evaluation-harness
uv pip install -e .[math]
uv pip install flash-attn==2.7.2.post1 --no-build-isolation
```

## Common Commands

```bash
# Run pretraining with contaminated corpus
python scripts/pretrain_language_model.py

# Multi-GPU pretraining
torchrun --standalone --nproc_per_node=1 scripts/pretrain_language_model.py

# Supervised fine-tuning
python scripts/sft_language_model.py

# Model evaluation (uses vLLM for inference)
python scripts/eval_language_model.py

# Teacher-forced evaluation (log probabilities of ground-truth solutions)
python scripts/eval_language_model_teacher_forcing.py

# Run W&B sweep
wandb sweep sweeps/pt/math_82gb_1xOT/model=qwen3-34M-1xOT.yaml
wandb agent [agent-id]

# Format code
black .
```

## Architecture

### Core Modules (`src/`)

- **globals.py**: Default configurations for pretraining, SFT, and evaluation. Contains `DEFAULT_PRETRAINING_CONFIG`, `DEFAULT_SUPERVISED_FINETUNING_CONFIG`, `DEFAULT_EVALUATION_CONFIG`, `DEFAULT_TEACHER_FORCING_EVALUATION_CONFIG`. Key contamination parameters: `benchmark_subset_fraction`, `num_benchmark_replicas_per_epoch`.

- **models.py**: Model loading and creation. `create_causalm_for_pretraining()` creates Qwen3 models from scratch; `load_automodelforcausallm()` loads from HF Hub. Supports Qwen3 models from 34M to 1.44B parameters.

- **data.py**: Dataset loading and preprocessing. `create_dataset_for_pretraining()` creates contaminated pretraining datasets by replicating MATH test set N times into fineweb-edu-dedup corpus. `create_dataset_for_supervised_finetuning()` handles MATH/GSM8K data.

- **analyze.py**: Analysis utilities for extracting metrics from W&B runs into pandas DataFrames.

- **plot.py**: Publication-quality matplotlib/seaborn visualizations.

- **neural_scaling_laws.py**: `PowerLawScalingFitter` class for fitting neural scaling laws. Fits: L(C,R) = E(R) + C_0(R) * C^(-α(R)).

### Training Scripts (`scripts/`)

- **pretrain_language_model.py**: Main pretraining with HF Trainer, DDP, W&B integration. Model naming convention: `mem_[modelname]_[benchmark]_rep_[replicas]_sbst_[subset]_epch_[epochs]_ot_[overtrain]`. Auto-uploads to HF Hub.

- **sft_language_model.py**: SFT using TRL's SFTTrainer. Can train on train or test split.

- **eval_language_model.py**: Evaluation using vLLM for inference and math-verify for scoring. Supports greedy decoding and sampling.

- **eval_language_model_teacher_forcing.py**: Teacher-forced evaluation that computes log probabilities of ground-truth solutions without sampling. Useful for measuring memorization since memorized solutions will have higher log probabilities.

### Experiment Sweeps (`sweeps/`)

W&B sweep configurations organized by experiment type:
- `pt/`: Pretraining sweeps (grid searches over contamination levels, model sizes)
- `sft/`: SFT sweeps
- `eval_pt/`, `eval_sft/`: Generative evaluation sweeps (sampling-based)
- `eval_pt_teacher_forcing/`: Teacher-forced evaluation sweeps (log probability-based)
- `dose_response/`: Dose response studies

### Analysis Notebooks (`notebooks/`)

Key notebook series:
- `10_*`, `11_*`: Pretraining cross-entropy and math verify results
- `12_*`, `13_*`: SFT cross-entropy and math verify results
- `20_*`: Contamination vs compute scaling analysis
- `30_*`: Dose response curves

### Manuscript (`manuscript/`)

LaTeX source for ICML paper. Main file is `00_main.tex`. Figures generated by notebooks go in `figures/`.

## Key Concepts

**Contamination Control**: The codebase controls contamination by injecting N replicas of the MATH test set into the pretraining corpus. Key parameters in `globals.py`:
- `num_benchmark_replicas_per_epoch`: Number of times test set is repeated
- `benchmark_subset_fraction`: Fraction of benchmark to use

**Model Naming**: Pretrained models follow the pattern `mem_[model]_[benchmark]_rep_[replicas]_sbst_[subset]_epch_[epochs]_ot_[overtrain]` for tracking contamination levels.

**Math Verify**: The project includes a fix for a critical bug in EleutherAI's math-verify implementation. The evaluation script uses a corrected version.

## Visual Aesthetic

All plots must follow these conventions (defined in `src/plot.py`):

**Global settings** (automatically applied via `import src.plot`):
- Style: `sns.set_style("whitegrid")`
- LaTeX rendering enabled with Computer Modern serif font
- Font size: 23
- Grid alpha: 0.5, showing both major and minor gridlines
- Default figure size: `src.plot.default_figsize` (10.67 × 8 inches)

**Plot construction pattern**:
```python
plt.close()
plt.figure(figsize=src.plot.default_figsize)
g = sns.lineplot(...)  # or sns.scatterplot
g.set(xlabel=..., ylabel=...)
src.plot.save_plot_with_multiple_extensions(plot_dir=results_dir, plot_filename="y=response_x=predictor_hue=variable")
# plt.show()  # DO NOT call plt.show() - it blocks execution and annoys the user
```

**Guidelines**:
- **NEVER call `plt.show()`** - it blocks script execution and opens interactive windows. Always comment it out or omit it entirely. Plots are saved to files via `save_plot_with_multiple_extensions()`.
- **ALWAYS visually inspect generated plots** - After generating any plot, use the Read tool to view the PNG file and scrutinize it closely. Check for: legends obscuring data or titles, axis labels being cut off, overlapping text, incorrect scales, missing data, or any other visual issues. Fix any problems before considering the task complete.
- No figure titles or axes titles (column/row titles in faceted plots are acceptable)
- Choose axis scales thoughtfully: use log/symlog when data spans orders of magnitude, linear otherwise
- Axis labels should use LaTeX math mode where appropriate
- Use `LogNorm()` for hue variables that span orders of magnitude (e.g., model size, FLOP)
- Format large numbers in legends with `format_g_legend_to_millions_and_billions()` or `format_g_legend_in_scientific_notation()`
- Filename convention: `y=<response>_x=<predictor>_hue=<grouping>`
- Save via `save_plot_with_multiple_extensions()` (outputs PDF and PNG at 300 DPI)

**Color palette consistency** (CRITICAL - colors must match across all notebooks):
- **Model size / Num. Parameters**: Always use `palette="flare"` with `LogNorm`. For seaborn plots with numeric hue, use `hue="Num. Parameters"` with `hue_norm=LogNorm(vmin=min_val, vmax=max_val)`. For manual plotting with string labels (e.g., "34M", "62M"), sample the colormap at LogNorm positions:
  ```python
  from matplotlib.colors import LogNorm
  param_values = [src.globals.MODEL_NAMES_TO_PARAMETERS_DICT[p] for p in unique_params]
  num_parameters_log_norm = LogNorm(vmin=min(param_values), vmax=max(param_values))
  flare_cmap = plt.cm.get_cmap("flare")
  params_palette = {
      p: flare_cmap(num_parameters_log_norm(src.globals.MODEL_NAMES_TO_PARAMETERS_DICT[p]))
      for p in unique_params
  }
  ```
- **Num. Replicas**: Use `palette="viridis"` with `SymLogNorm(linthresh=1.0)` for numeric hue
- **Temperature**: Use `palette="YlOrBr_r"`
- **Before creating any new color palette**, check existing notebooks (especially `notebooks/11_*`) to ensure colors match. Cross-notebook color consistency is essential for publication.

**IMPORTANT: Use SymLogNorm for replica colors, not discrete indexing**:
- Notebooks 10/11 use `SymLogNorm(linthresh=1.0)` with `palette="viridis"` to map replica values to colors continuously.
- For manual plotting (not seaborn), replicate this by sampling the colormap at SymLogNorm positions:
  ```python
  from matplotlib.colors import SymLogNorm
  import matplotlib

  all_replicas = [0, 1, 3, 10, 32, 100, 316, 1000, 3162]
  replica_sym_norm = SymLogNorm(linthresh=1.0, vmin=0, vmax=max(all_replicas))
  viridis_cmap = matplotlib.colormaps["viridis"]
  R_to_color = {r: viridis_cmap(replica_sym_norm(r)) for r in all_replicas}
  ```
- **WRONG** (discrete indexing gives different colors):
  ```python
  colors = sns.color_palette("viridis", len(all_replicas))
  palette = {r: colors[i] for i, r in enumerate(all_replicas)}
  ```
- **CORRECT** (SymLogNorm gives consistent colors):
  ```python
  replica_sym_norm = SymLogNorm(linthresh=1.0, vmin=0, vmax=3162)
  viridis_cmap = matplotlib.colormaps["viridis"]
  palette = {r: viridis_cmap(replica_sym_norm(r)) for r in all_replicas}
  ```

**Legend placement**:
- Use ONE legend per figure, not redundant legends on each subplot
- Place legends where they don't obscure data (often lower-left or outside the plot area)
- For multi-panel figures, use `fig.legend()` with appropriate `bbox_to_anchor` positioning, or place in a single subplot's whitespace area

## W&B Integration

All experiments log to Weights & Biases. Ensure `WANDB_API_KEY` is set. Sweep configs in `sweeps/` define hyperparameter grids for systematic experiments.

## HuggingFace Hub

Trained models are automatically uploaded to HF Hub at the end of training. Requires `HF_TOKEN` environment variable.

## Output Conventions

**Reports and documentation**:
- Always use Markdown (`.md`) for reports, analysis summaries, and documentation
- Never generate duplicate report files (e.g., both `.txt` and `.md` versions)
- Place reports in the notebook's `results/` directory with descriptive names (e.g., `MODEL_FITTING_REPORT.md`, `NLL_UPTICK_ANALYSIS.md`)

**Generated files**:
- Each notebook should generate a single, canonical version of each output
- If refactoring code that generates reports, consolidate into one well-formatted file rather than creating new files alongside old ones
- Delete obsolete output files when their generating code is removed

## Pre-Commit Cleanup Checklist

Before considering any task complete, review the repository state:

1. **Run `git status`** to see all modified, deleted, and untracked files
2. **Check for redundant files**: Are there duplicate outputs (e.g., `report.txt` AND `report.md`)? Delete the obsolete one.
3. **Check for orphaned files**: Did you delete code that generated certain outputs? Delete those outputs too.
4. **Verify outputs are current**: Re-run notebooks/scripts if needed to ensure outputs match the current code
5. **Review untracked files**: Should new files be committed or added to `.gitignore`?
6. **Test the code**: Run the modified scripts to verify they execute without errors
