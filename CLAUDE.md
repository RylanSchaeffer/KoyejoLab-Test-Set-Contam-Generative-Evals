# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Research project investigating how test set contamination (data leakage) affects generative model evaluations on math problem-solving benchmarks. The project studies whether generative evaluations respond differently to contamination compared to discriminative evaluations, using controlled contamination experiments with Qwen3 models.

## Environment Setup

```bash
# Install uv package manager
conda install conda-forge::uv

# Create virtual environment
uv venv -p 3.11.5 mem_scoring_vs_sampling_env
source mem_scoring_vs_sampling_env/bin/activate

# Install dependencies
uv pip install -r requirements.txt

# Install EleutherAI LM Evaluation Harness (required for math-verify)
git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness
cd lm-evaluation-harness
uv pip install -e .[math]
uv pip install flash-attn==2.7.2.post1 --no-build-isolation
```

## Common Commands

```bash
# Run pretraining with contaminated corpus
python scripts/pretrain_language_model.py

# Multi-GPU pretraining
torchrun --standalone --nproc_per_node=1 scripts/pretrain_language_model.py

# Supervised fine-tuning
python scripts/sft_language_model.py

# Model evaluation (uses vLLM for inference)
python scripts/eval_language_model.py

# Teacher-forced evaluation (log probabilities of ground-truth solutions)
python scripts/eval_language_model_teacher_forcing.py

# Run W&B sweep
wandb sweep sweeps/pt/math_82gb_1xOT/model=qwen3-34M-1xOT.yaml
wandb agent [agent-id]

# Format code
black .
```

## Architecture

### Core Modules (`src/`)

- **globals.py**: Default configurations for pretraining, SFT, and evaluation. Contains `DEFAULT_PRETRAINING_CONFIG`, `DEFAULT_SUPERVISED_FINETUNING_CONFIG`, `DEFAULT_EVALUATION_CONFIG`, `DEFAULT_TEACHER_FORCING_EVALUATION_CONFIG`. Key contamination parameters: `benchmark_subset_fraction`, `num_benchmark_replicas_per_epoch`.

- **models.py**: Model loading and creation. `create_causalm_for_pretraining()` creates Qwen3 models from scratch; `load_automodelforcausallm()` loads from HF Hub. Supports Qwen3 models from 34M to 1.44B parameters.

- **data.py**: Dataset loading and preprocessing. `create_dataset_for_pretraining()` creates contaminated pretraining datasets by replicating MATH test set N times into fineweb-edu-dedup corpus. `create_dataset_for_supervised_finetuning()` handles MATH/GSM8K data.

- **analyze.py**: Analysis utilities for extracting metrics from W&B runs into pandas DataFrames.

- **plot.py**: Publication-quality matplotlib/seaborn visualizations.

- **neural_scaling_laws.py**: `PowerLawScalingFitter` class for fitting neural scaling laws. Fits: L(C,R) = E(R) + C_0(R) * C^(-Î±(R)).

### Training Scripts (`scripts/`)

- **pretrain_language_model.py**: Main pretraining with HF Trainer, DDP, W&B integration. Model naming convention: `mem_[modelname]_[benchmark]_rep_[replicas]_sbst_[subset]_epch_[epochs]_ot_[overtrain]`. Auto-uploads to HF Hub.

- **sft_language_model.py**: SFT using TRL's SFTTrainer. Can train on train or test split.

- **eval_language_model.py**: Evaluation using vLLM for inference and math-verify for scoring. Supports greedy decoding and sampling.

- **eval_language_model_teacher_forcing.py**: Teacher-forced evaluation that computes log probabilities of ground-truth solutions without sampling. Useful for measuring memorization since memorized solutions will have higher log probabilities.

### Experiment Sweeps (`sweeps/`)

W&B sweep configurations organized by experiment type:
- `pt/`: Pretraining sweeps (grid searches over contamination levels, model sizes)
- `sft/`: SFT sweeps
- `eval_pt/`, `eval_sft/`: Generative evaluation sweeps (sampling-based)
- `eval_pt_teacher_forcing/`: Teacher-forced evaluation sweeps (log probability-based)
- `dose_response/`: Dose response studies

### Analysis Notebooks (`notebooks/`)

Key notebook series:
- `10_*`, `11_*`: Pretraining cross-entropy and math verify results
- `12_*`, `13_*`: SFT cross-entropy and math verify results
- `20_*`: Contamination vs compute scaling analysis
- `30_*`: Dose response curves

### Manuscript (`manuscript/`)

LaTeX source for ICML paper. Main file is `00_main.tex`. Figures generated by notebooks go in `figures/`.

## Key Concepts

**Contamination Control**: The codebase controls contamination by injecting N replicas of the MATH test set into the pretraining corpus. Key parameters in `globals.py`:
- `num_benchmark_replicas_per_epoch`: Number of times test set is repeated
- `benchmark_subset_fraction`: Fraction of benchmark to use

**Model Naming**: Pretrained models follow the pattern `mem_[model]_[benchmark]_rep_[replicas]_sbst_[subset]_epch_[epochs]_ot_[overtrain]` for tracking contamination levels.

**Math Verify**: The project includes a fix for a critical bug in EleutherAI's math-verify implementation. The evaluation script uses a corrected version.

## W&B Integration

All experiments log to Weights & Biases. Ensure `WANDB_API_KEY` is set. Sweep configs in `sweeps/` define hyperparameter grids for systematic experiments.

## HuggingFace Hub

Trained models are automatically uploaded to HF Hub at the end of training. Requires `HF_TOKEN` environment variable.
