command:
  - ${env}
  - torchrun
  - --standalone
  - --nproc_per_node=1
  - ${program}
  - ${args}
program: scripts/pretrain_language_model.py
entity: jkazdan
project: memorization-scoring-vs-sampling-pt-param-switch
method: grid
parameters:
  data_config:
    parameters:
      do_not_displace:
        values: [False]
      corpus:
        values: ["fineweb-edu-dedup"]
      use_corpus_for_replication:
        values: [True]
      memorization_budget_fraction:
        values: [ 0.10 ]
      benchmark:
        values: [ "EleutherAI/minerva_math" ]
      benchmark_subset_fraction:
        values: [ 0.000001, 0.00001, 0.0001, 0.001, 0.01 ]
      benchmark_shuffle_seed:
        values: [ 0 ]
      num_benchmark_replicas_per_epoch:
        values: [ -1 ]
      shuffle_seed:
        values: [0]
  model_config:
    parameters:
      attn_implementation:
        values: [ "flash_attention_2" ]
      model_name:
        values: [ "Qwen3/Qwen3-93M" ]
      torch_dtype:
        values: [ "bfloat16" ]
  trainer_config:
    parameters:
      data_seed:
        values: [ 0 ]
      dataloader_drop_last:
        values: [ True ]
      dataloader_num_workers:
        values: [ 8 ]
      dataloader_prefetch_factor:
        values: [ 4 ]
      eval_on_start:
        values: [ True ]
      eval_strategy:
        values: [ steps ]
      eval_steps:
        values: [ 2000 ]
      gradient_checkpointing:
        values: [ False ]
      hub_strategy:
        values: ["end"]
      base_learning_rate:
        values: [ 0.000001 ]
      logging_steps:
        values: [ 1 ]
      lr_scheduler_type:
        values: [ "cosine" ]
      max_grad_norm:
        values: [ 1.0 ]
      max_length:
        values: [ 2048 ]
      max_steps:
        values: [ -1 ]
      num_train_epochs:
        values: [ 1 ]
      optim:
        values: [ "adamw_torch_fused" ]
      adam_beta1:
        values: [ 0.9 ]
      adam_beta2:
        values: [ 0.95 ]
      overtrain_multiplier:
        values: [ 1.0 ]
      per_device_eval_batch_size:
        values: [ 34 ]
      per_device_train_batch_size:
        values: [ 34 ]
      remove_unused_columns:
        values: [ True ]
      report_to:
        values: ["wandb"]
      save_strategy:
        values: [ "no" ]
      save_total_limit:
        values: [ 1 ]
      torch_compile:
        values: [ True ]
      warmup_steps:
        values: [ 250 ]
      warmup_ratio:
        values: [0.2]
      weight_decay:
        values: [ 0.0 ]
  seed:
    values: [0]
